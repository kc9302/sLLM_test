{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2c305c-4366-459e-a06e-bc2b086c1091",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Qwen2-7B-Instruct-bnb-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb29598-f88d-4fba-ace4-8ce0da3398fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델 초기화\n",
    "model_id = \"unsloth/Qwen2-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d0d9d-7ae1-49f6-965d-b44bdf71cf69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문제 불러오기\n",
    "english_1 = \"\"\"\n",
    "빈칸에 알맞는 것을 고르시오.\n",
    "The founding population of our direct ancestors is not\n",
    "thought to have been much larger than 2,000 individuals;\n",
    "some think the group was as small as a few hundred. How,\n",
    "then, did we go from such a fragile minority population to a\n",
    "tide of humanity 7 billion strong and growing? There is only\n",
    "one way, according to Richard Potts. You give up on\n",
    "___________. You don't try to beat back the changes. You\n",
    "begin to care about consistency within a given habitate,\n",
    "because such consistency isn't an option. You adapt to\n",
    "variation itself. It was a brilliant strategy. Instead of\n",
    "learning how to survive in just one or two ecological\n",
    "environments, we took on the entire globe.\n",
    "\n",
    "1번 \"stability\" 2번 \"morality\" 3번 \"fairness\" 4번 \"reputation\" 5번 \"challenges\"\n",
    "\"\"\"\n",
    "\n",
    "english_2 = \"\"\"대화의 빈 칸에 알맞은 것을 고르세요.\n",
    "A : Happy birthday!\n",
    "B : Oh, ____________\n",
    "\n",
    "1번 \"sorry\" 2번 \"thank you\" 3번 \"June first\"\n",
    " \"\"\"\n",
    "\n",
    "english_3 = \"\"\"주어진 단어에 포함되는 단어를 모두 고르시오.\n",
    "주어진 단어 : \"Pet\"\n",
    "1번 \"cat\" 2번 \"picture\" 3번 \"rabbit\" 4번 \"toothpaste\"\n",
    "\"\"\"\n",
    "\n",
    "korean_1 = \"\"\"동물 친구들이 곰에게 글을 읽을 때의 바른 자세에 대하여 말하고 있습니다. 토끼는 의자를 당겨서 앉아야 한다고 하였습니다. 기린은 허리를 곧게 펴야 한다고 하였습니다. 생쥐는 책과 눈의 거리를 알맞게 해야 한다고 하였습니다.\n",
    "\n",
    "1. 토끼가 곰에게 해 준 말은 무엇입니까?\n",
    "1번 \"허리를 곧게 펴야 한다.\" 2번 \"의자를 당겨서 앉아야 한다.\" 3번 \"다리를 쭉 펴고 읽어야 한다.\" 4번 \"책상 위에 엎드려 읽어야 한다.\" 5번 \"책과 팔의 거리를 알맞게 해야 한다.\"\n",
    "\"\"\"\n",
    "\n",
    "korean_2 = \"\"\"다음 중 자음자를 고르세요.\n",
    "1번 \"ㅏ\" 2번 \"ㅣ\" 3번 \"ㅁ\" 4번 \"ㅜ\" 5번 \"ㅛ\"\n",
    "\"\"\"\n",
    "\n",
    "korean_3 = \"\"\"나무 나무 무슨 나무\n",
    "가자 가자 감나무\n",
    "배가 아파 배나무\n",
    "바람 솔솔 _______\n",
    "\n",
    "빈칸에 들어갈 말로 가장 알맞는 것은 무엇입니까?\n",
    "1번 \"소나무\" 2번 \"감나무\" 3번 \"배나무\" 4번 \"사과나무\" 5번 \"포도나무\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860af5d8-0992-4fc5-b207-76a27ed2e00a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d69ddb-021c-4c1a-a41f-6f669c05d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75af2b6-ea91-4272-8912-0a7c0a2f3f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6e638-dc81-4dcd-b29c-0c54ef3bfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff189b-040d-488c-ad1c-81b1e56f7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0beac-e263-4201-a301-5e1efe7c9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c248b-5541-4903-9571-61fa42a3cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e39b2-c305-47c2-8109-6e3fcefcc6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3931b3-40e6-4a85-aaab-57ff23e0a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2af30-a16c-4f91-a7b7-09117666f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf1723-c169-4ccf-b408-64e098b562f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621ee36-ad6a-4461-b831-588f3872f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7b52-4fd4-4152-a668-1f20602fc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba999c90-3da8-41b1-bf59-d6c57c473d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cacf25-2cf4-4f8d-890c-78ad87fe7fab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f9042-e696-46c4-ac6b-ca76fc04e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ba8b3-0228-4006-b6a5-bd845131bc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 512 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "model_id = \"unsloth/Qwen2-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86c781-fef7-47c9-861a-63c4b83972d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3500c-6cd5-42e4-a22e-624930bee622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6af5d8-5c0a-465e-979f-e54495639bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312]",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
