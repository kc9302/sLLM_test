{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefab32a-b47d-4c86-8a3d-e6cc0be0f182",
   "metadata": {},
   "source": [
    "## Chocolatine-Fusion-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbb51a-e650-4ac6-b345-a633533e1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델 초기화\n",
    "model_id = \"FINGU-AI/Chocolatine-Fusion-14B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8380a-fa8f-4190-8cce-e733dd79b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 불러오기\n",
    "english_1 = \"\"\"\n",
    "빈칸에 알맞는 것을 고르시오.\n",
    "The founding population of our direct ancestors is not\n",
    "thought to have been much larger than 2,000 individuals;\n",
    "some think the group was as small as a few hundred. How,\n",
    "then, did we go from such a fragile minority population to a\n",
    "tide of humanity 7 billion strong and growing? There is only\n",
    "one way, according to Richard Potts. You give up on\n",
    "___________. You don't try to beat back the changes. You\n",
    "begin to care about consistency within a given habitate,\n",
    "because such consistency isn't an option. You adapt to\n",
    "variation itself. It was a brilliant strategy. Instead of\n",
    "learning how to survive in just one or two ecological\n",
    "environments, we took on the entire globe.\n",
    "\n",
    "1번 \"stability\" 2번 \"morality\" 3번 \"fairness\" 4번 \"reputation\" 5번 \"challenges\"\n",
    "\"\"\"\n",
    "\n",
    "english_2 = \"\"\"대화의 빈 칸에 알맞은 것을 고르세요.\n",
    "A : Happy birthday!\n",
    "B : Oh, ____________\n",
    "\n",
    "1번 \"sorry\" 2번 \"thank you\" 3번 \"June first\"\n",
    " \"\"\"\n",
    "\n",
    "english_3 = \"\"\"주어진 단어에 포함되는 단어를 모두 고르시오.\n",
    "주어진 단어 : \"Pet\"\n",
    "1번 \"cat\" 2번 \"picture\" 3번 \"rabbit\" 4번 \"toothpaste\"\n",
    "\"\"\"\n",
    "\n",
    "korean_1 = \"\"\"동물 친구들이 곰에게글을 읽을 때의 바른 자세에 대하여 말하고 있습니다. 토끼는 의자를 당겨서 앉아야 한다고 하였습니다. 기린은 허리를 곧게 펴야 한다고 하였습니다. 생쥐는 책과 눈의 거리를 알맞게 해야 한다고 하였습니다.\n",
    "\n",
    "1. 토끼가 곰에게 해 준 말은 무엇입니까?\n",
    "1번 \"허리를 곧게 펴야 한다.\" 2번 \"의자를 당겨서 앉아야 한다.\" 3번 \"다리를 쭉 펴고 읽어야 한다.\" 4번 \"책상 위에 엎드려 읽어야 한다.\" 5번 \"책과 팔의 거리를 알맞게 해야 한다.\"\n",
    "\"\"\"\n",
    "\n",
    "korean_2 = \"\"\"다음 중 자음자를 고르세요.\n",
    "1번 \"ㅏ\" 2번 \"ㅣ\" 3번 \"ㅁ\" 4번 \"ㅜ\" 5번 \"ㅛ\"\n",
    "\"\"\"\n",
    "\n",
    "korean_3 = \"\"\"나무 나무 무슨 나무\n",
    "가자 가자 감나무\n",
    "배가 아파 배나무\n",
    "바람 솔솔 _______\n",
    "\n",
    "빈칸에 들어갈 말로 가장 알맞는 것은 무엇입니까?\n",
    "1번 \"소나무\" 2번 \"감나무\" 3번 \"배나무\" 4번 \"사과나무\" 5번 \"포도나무\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b315d-ad76-49e8-8e51-f26799c22e1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89418eb-d39e-4fec-b8ea-0c966e4bca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0da91-4463-4ecf-910a-02f7f6c7cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21fe85-9986-446b-a50b-33caf1d5dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b64e4-a80e-448d-b2b9-94de69d6b272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32dbebc-29e5-427b-a228-db2fd5759522",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccbaa5-0eb7-4959-bcd5-c710afb6e0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9cbe5-0ab7-4755-ade2-60fba72174f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fc0c3-cbb8-4c76-a7d3-10d91b81291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967511c5-c6f0-4829-929f-d64df7e80bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3fa87-c296-4271-a8cd-34f739c6e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8094a44-3d3f-43fd-8d91-8ecde5582194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dcff2-6f87-4900-810d-1e2687f48dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55cffd-1fcf-40c3-99dc-fc064dbeb54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9286c87-e32c-4d82-922a-8fe21c77376e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-Tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db885a-40d4-4fae-9edb-629a4e3273b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  {\n",
    "    \"문제\": \"빈칸에 알맞는 것을 고르시오.\\nThe founding population of our direct ancestors is not\\nthought to have been much larger than 2,000 individuals;\\nsome think the group was as small as a few hundred. How,\\nthen, did we go from such a fragile minority population to a\\ntide of humanity 7 billion strong and growing? There is only\\none way, according to Richard Potts. You give up on\\n___________. You don't try to beat back the changes. You\\nbegin to care about consistency within a given habitate,\\nbecause such consistency isn't an option. You adapt to\\nvariation itself. It was a brilliant strategy. Instead of\\nlearning how to survive in just one or two ecological\\nenvironments, we took on the entire globe.\\n\\n1번 \\\\\\\"stability\\\\\\\" 2번 \\\\\\\"morality\\\\\\\" 3번 \\\\\\\"fairness\\\\\\\" 4번 \\\\\\\"reputation\\\\\\\" 5번 \\\\\\\"challenges\\\\\\\"\",\n",
    "    \"정답해설\": \"정답은 \\\\\\\"stability\\\\\\\"입니다. 문장에서 \\\\\\\"You don't try to beat back the changes\\\\\\\"라는 부분을 보면 변화를 물리치려 하지 않는다는 뜻을 가지고 있습니다. 즉, 안정성을 추구하지 않고 변화를 받아들이는 것이 우리의 조상들이 번성하게 된 비결이라는 것을 의미합니다.\"\n",
    "  },\n",
    "  {\n",
    "    \"문제\": \"대화의 빈 칸에 알맞은 것을 고르세요.\\nA : Happy birthday!\\nB : Oh, ____________\\n\\n1번 \\\\\\\"sorry\\\\\\\" 2번 \\\\\\\"thank you\\\\\\\" 3번 \\\\\\\"June first\\\\\\\"\",\n",
    "    \"정답해설\": \"정답은 2번 \\\\\\\"thank you\\\\\\\"입니다.\\n\\n이 대화는 누군가의 생일을 축하하는 상황에서 나옵니다. A가 \\\\\\\"Happy birthday!\\\\\\\"이라고 말했으므로, B는 이를 칭찬하고 감사의 인사를 전해야 합니다. 따라서 \\\\\\\"thank you\\\\\\\"가 가장 적합한 대답입니다.\\n\\n\\\\\\\"Sorry\\\\\\\"는 잘못된 답입니다. 상대방이 생일을 축하해주었는데, \\\\\\\"죄송합니다\\\\\\\"라고 말하는 것은 자연스럽지 않습니다.\\n\\n\\\\\\\"June first\\\\\\\"는 생일 날짜를 표현하는 것으로, 이 대화에서 B가 날짜를 말하는 것은 관련이 없습니다.\\n\\n따라서 B는 \\\\\\\"Oh, thank you\\\\\\\"라고 대답하는 것이 가장 자연스럽고 적절합니다.\"\n",
    "  },\n",
    "  {\n",
    "    \"문제\": \"주어진 단어에 포함되는 단어를 모두 고르시오.\\n주어진 단어 : \\\\\\\"Pet\\\\\\\" 1번 \\\\\\\"cat\\\\\\\" 2번 \\\\\\\"picture\\\\\\\" 3번 \\\\\\\"rabbit\\\\\\\" 4번 \\\\\\\"toothpaste\\\\\\\"\",\n",
    "    \"정답해설\": \"학생, 주어진 단어 \\\\\\\"Pet\\\\\\\"에서 포함되는 단어를 고르라는 문제를 해결해 보자. 주의 깊게 주어진 단어와 후보 단어들을 비교해 보자.\\n\\n\\\\\\\"Pet\\\\\\\"이라는 단어에서 포함되는 다른 단어는 \\\\\\\"cat\\\\\\\"과 \\\\\\\"rabbit\\\\\\\"이다. \\\\\\\"cat\\\\\\\"은 \\\\\\\"Pet\\\\\\\"의 마지막 두 글자 \\\\\\\"t\\\\\\\"와 \\\\\\\"c\\\\\\\"를 포함하고 있고, \\\\\\\"rabbit\\\\\\\"은 \\\\\\\"Pet\\\\\\\"의 마지막 글자 \\\\\\\"t\\\\\\\"를 포함하고 있다.\\n\\n반면에, \\\\\\\"picture\\\\\\\"는 \\\\\\\"Pet\\\\\\\"의 어떤 글자도 포함하지 않고, \\\\\\\"toothpaste\\\\\\\"는 \\\\\\\"Pet\\\\\\\"의 마지막 글자 \\\\\\\"t\\\\\\\"를 포함하고 있지만 다른 공통점이 없다.\\n\\n결론적으로, 주어진 \\\\\\\"Pet\\\\\\\" 단어에 포함되는 단어는 1번 \\\\\\\"cat\\\\\\\"과 3번 \\\\\\\"rabbit\\\\\\\"이다.\"\n",
    "  },\n",
    "  {\n",
    "    \"문제\": \"동물 친구들이 곰에게글을 읽을 때의 바른 자세에 대하여 말하고 있습니다. 토끼는 의자를 당겨서 앉아야 한다고 하였습니다. 기린은 허리를 곧게 펴야 한다고 하였습니다. 생쥐는 책과 눈의 거리를 알맞게 해야 한다고 하였습니다.\\n\\n1. 토끼가 곰에게 해 준 말은 무엇입니까?\\n1번 \\\\\\\"허리를 곧게 펴야 한다.\\\\\\\" 2번 \\\\\\\"의자를 당겨서 앉아야 한다.\\\\\\\" 3번 \\\\\\\"다리를 쭉 펴고 읽어야 한다.\\\\\\\" 4번 \\\\\\\"책상 위에 엎드려 읽어야 한다.\\\\\\\" 5번 \\\\\\\"책과 팔의 거리를 알맞게 해야 한다.\\\\\\\"\",\n",
    "    \"정답해설\": \"문제의 정답은 2번 \\\\\\\"의자를 당겨서 앉아야 한다.\\\\\\\"입니다.\\n\\n토끼가 곰에게 조언한 내용은 \\\\\\\"의자를 당겨서 앉아야 한다\\\\\\\"는 것이므로 이에 해당하는 2번이 맞습니다.\\n\\n각 동물들이 제시한 바른 자세에 대해 설명하자면:\\n- 토끼는 의자를 당겨서 앉는 것이 중요하다고 조언했습니다. 이는 편안한 자세를 유지하고 척추를 곧게 펴는 데 도움이 됩니다.\\n- 기린은 허리를 곧게 펴야 한다고 말했습니다. 이는 장시간 앉아 있을 때 목과 등의 피로를 줄이는 데 도움이 됩니다.\\n- 생쥐는 책과 눈의 거리를 알맞게 유지해야 한다고 조언했습니다. 이는 시력 보호와 피로 감소에 도움이 됩니다.\\n\\n따라서 토끼가 곰에게 한 조언은 2번 \\\\\\\"의자를 당겨서 앉아야 한다\\\\\\\"가 맞습니다.\"\n",
    "  },\n",
    "  {\n",
    "    \"문제\": \"다음 중 자음자를 고르세요.\\n1번 \\\\\\\"ㅏ\\\\\\\" 2번 \\\\\\\"ㅣ\\\\\\\" 3번 \\\\\\\"ㅁ\\\\\\\" 4번 \\\\\\\"ㅜ\\\\\\\" 5번 \\\\\\\"ㅛ\\\\\\\"\",\n",
    "    \"정답해설\": \"교실에서 선생님처럼 학생에게 설명하겠습니다.\\n\\n문제에서 주어진 단어들 중 자음자를 고르라는 요청이 있습니다. 자음자는 한국어의 초성에 해당하는 글자입니다.\\n\\n1번 \\\\\\\"ㅏ\\\\\\\"는 모음자입니다. \\n2번 \\\\\\\"ㅣ\\\\\\\"는 모음자입니다.\\n3번 \\\\\\\"ㅁ\\\\\\\"은 자음자입니다.\\n4번 \\\\\\\"ㅜ\\\\\\\"는 모음자입니다.\\n5번 \\\\\\\"ㅛ\\\\\\\"는 종성에 해당합니다.\\n\\n따라서 올바른 답은 3번 \\\\\\\"ㅁ\\\\\\\"입니다.\"\n",
    "  },\n",
    "  {\n",
    "    \"문제\": \"나무 나무 무슨 나무\\n가자 가자 감나무\\n배가 아파 배나무\\n바람 솔솔 _______\\n\\n빈칸에 들어갈 말로 가장 알맞는 것은 무엇입니까?\\n1번 \\\\\\\"소나무\\\\\\\" 2번 \\\\\\\"감나무\\\\\\\" 3번 \\\\\\\"배나무\\\\\\\" 4번 \\\\\\\"사과나무\\\\\\\" 5번 \\\\\\\"포도나무\\\\\\\"\",\n",
    "    \"정답해설\": \"정답은 1번 \\\\\\\"소나무\\\\\\\"입니다.\\n\\n이 노래는 다양한 나무들을 언급하며 재미있는 표현을 사용하고 있습니다. \\\\\\\"바람 솔솔\\\\\\\" 이라는 표현을 보면 바람이 부는 것을 상기시킵니다. 바람이 부는 장면에서 가장 먼저 떠오르는 나무는 \\\\\\\"소나무\\\\\\\"입니다. 소나무는 바람이 부어도 굳건하게 서있는 모습이 고결해 보이기 때문입니다. 또한, \\\\\\\"감나무\\\\\\\"나 \\\\\\\"배나무\\\\\\\"는 바람과 직접적인 연관성이 약하기 때문에 \\\\\\\"소나무\\\\\\\"가 가장 적합한 표현입니다.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "result = {}\n",
    "result[\"문제\"] = [item[\"문제\"] for item in data]\n",
    "result[\"정답해설\"] = [item[\"정답해설\"] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b33570-3392-4b8a-b006-d7f451a1a76e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d479b-75b9-46ae-956f-bf75660a79d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"FINGU-AI/Chocolatine-Fusion-14B\"\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = str(self.questions[idx])\n",
    "        answer = str(self.answers[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "     \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# 토크나이저 및 데이터셋 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "max_length = 512  # 적절한 최대 길이 설정\n",
    "dataset = QADataset(result['문제'], result['정답해설'], tokenizer, max_length)\n",
    "\n",
    "# DataLoader 생성\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=False) #배치사이즈는 필요에 따라 조절가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee325a3-dbe4-4402-9cb2-e5b1d612fe60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 모델 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b0048-d9af-48a5-babf-24832e8a1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6629625-c173-4b56-a9f7-bc465e00bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "model.train()\n",
    "for input_ids, attention_mask in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# # 모델 저장\n",
    "# model.save_pretrained(\"./fine_tuned_model\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_lightning]",
   "language": "python",
   "name": "conda-env-torch_lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
