{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2c305c-4366-459e-a06e-bc2b086c1091",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb29598-f88d-4fba-ace4-8ce0da3398fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac049c1c67046fd8fd7ef88e3a7710b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c345555f5604c01968793881b9c8b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40100b5de2d141169e6e7796eb7c8a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ceb452b4014067aa8d789d6c831872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8accc69be5f427eb1a4ba6bca7b01c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac61a950589477e9792863c000ca478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델 초기화\n",
    "model_id = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056d0d9d-7ae1-49f6-965d-b44bdf71cf69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문제 불러오기\n",
    "english_1 = \"\"\"\n",
    "빈칸에 알맞는 것을 고르시오.\n",
    "The founding population of our direct ancestors is not\n",
    "thought to have been much larger than 2,000 individuals;\n",
    "some think the group was as small as a few hundred. How,\n",
    "then, did we go from such a fragile minority population to a\n",
    "tide of humanity 7 billion strong and growing? There is only\n",
    "one way, according to Richard Potts. You give up on\n",
    "___________. You don't try to beat back the changes. You\n",
    "begin to care about consistency within a given habitate,\n",
    "because such consistency isn't an option. You adapt to\n",
    "variation itself. It was a brilliant strategy. Instead of\n",
    "learning how to survive in just one or two ecological\n",
    "environments, we took on the entire globe.\n",
    "\n",
    "1번 \"stability\" 2번 \"morality\" 3번 \"fairness\" 4번 \"reputation\" 5번 \"challenges\"\n",
    "\"\"\"\n",
    "\n",
    "english_2 = \"\"\"대화의 빈 칸에 알맞은 것을 고르세요.\n",
    "A : Happy birthday!\n",
    "B : Oh, ____________\n",
    "\n",
    "1번 \"sorry\" 2번 \"thank you\" 3번 \"June first\"\n",
    " \"\"\"\n",
    "\n",
    "english_3 = \"\"\"주어진 단어에 포함되는 단어를 모두 고르시오.\n",
    "주어진 단어 : \"Pet\"\n",
    "1번 \"cat\" 2번 \"picture\" 3번 \"rabbit\" 4번 \"toothpaste\"\n",
    "\"\"\"\n",
    "\n",
    "korean_1 = \"\"\"동물 친구들이 곰에게 글을 읽을 때의 바른 자세에 대하여 말하고 있습니다. 토끼는 의자를 당겨서 앉아야 한다고 하였습니다. 기린은 허리를 곧게 펴야 한다고 하였습니다. 생쥐는 책과 눈의 거리를 알맞게 해야 한다고 하였습니다.\n",
    "\n",
    "1. 토끼가 곰에게 해 준 말은 무엇입니까?\n",
    "1번 \"허리를 곧게 펴야 한다.\" 2번 \"의자를 당겨서 앉아야 한다.\" 3번 \"다리를 쭉 펴고 읽어야 한다.\" 4번 \"책상 위에 엎드려 읽어야 한다.\" 5번 \"책과 팔의 거리를 알맞게 해야 한다.\"\n",
    "\"\"\"\n",
    "\n",
    "korean_2 = \"\"\"다음 중 자음자를 고르세요.\n",
    "1번 \"ㅏ\" 2번 \"ㅣ\" 3번 \"ㅁ\" 4번 \"ㅜ\" 5번 \"ㅛ\"\n",
    "\"\"\"\n",
    "\n",
    "korean_3 = \"\"\"나무 나무 무슨 나무\n",
    "가자 가자 감나무\n",
    "배가 아파 배나무\n",
    "바람 솔솔 _______\n",
    "\n",
    "빈칸에 들어갈 말로 가장 알맞는 것은 무엇입니까?\n",
    "1번 \"소나무\" 2번 \"감나무\" 3번 \"배나무\" 4번 \"사과나무\" 5번 \"포도나무\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860af5d8-0992-4fc5-b207-76a27ed2e00a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d69ddb-021c-4c1a-a41f-6f669c05d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75af2b6-ea91-4272-8912-0a7c0a2f3f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>\n",
      "빈칸에 알맞는 것을 고르시오.\n",
      "The founding population of our direct ancestors is not\n",
      "thought to have been much larger than 2,000 individuals;\n",
      "some think the group was as small as a few hundred. How,\n",
      "then, did we go from such a fragile minority population to a\n",
      "tide of humanity 7 billion strong and growing? There is only\n",
      "one way, according to Richard Potts. You give up on\n",
      "___________. You don't try to beat back the changes. You\n",
      "begin to care about consistency within a given habitate,\n",
      "because such consistency isn't an option. You adapt to\n",
      "variation itself. It was a brilliant strategy. Instead of\n",
      "learning how to survive in just one or two ecological\n",
      "environments, we took on the entire globe.\n",
      "\n",
      "1번 \"stability\" 2번 \"morality\" 3번 \"fairness\" 4번 \"reputation\" 5번 \"challenges\"\n",
      "Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, so I'm trying to figure out this question. The sentence is asking how we went from a small population of a few hundred or a couple thousand ancestors to a huge population of seven billion people today. The options given are five words: stability, morality, fairness, reputation, and challenges.\n",
      "\n",
      "The passage says that Richard Potts thinks the answer is about not trying to fight changes but instead focusing on consistency within a habitat and adapting to variations. It also mentions that by doing this, we didn't just adapt to one or two environments but the whole globe.\n",
      "\n",
      "So, I need to figure out which of those five words fits best as the blank. Let me think about each option.\n",
      "\n",
      "Stability: This makes sense because the passage talks about consistency within a habitat. Stability is about being consistent, so maybe this is the right choice.\n",
      "\n",
      "Moral: The passage doesn't mention anything about ethics or right and wrong, so I don't think morality is the answer.\n",
      "\n",
      "Fairness: Again, the passage doesn't discuss fairness or justice, so I can rule that out.\n",
      "\n",
      "Reputation: That's more about how people are seen by others, which isn't really connected to population growth.\n",
      "\n",
      "Challenges: The passage mentions challenges in a general sense, but it's more about adapting to changes rather than facing specific challenges.\n",
      "\n",
      "Putting it all together, stability seems to fit because it's about being consistent and adapting, which allowed us to expand our population across various environments.\n",
      "</think>\n",
      "\n",
      "The correct answer is \"stability.\" The passage explains that by maintaining consistency within their habitat and adapting to variations, our ancestors were able to expand their population across the globe. This focus on stability allowed them to thrive and grow into a large population.\n",
      "\n",
      "**Answer: stability**  \n",
      "**Explanation:** The passage highlights that our ancestors focused on maintaining stability within their environment and adapting to variations, which enabled their population to grow from a small number to a large one.\n",
      "16.27441 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d6e638-dc81-4dcd-b29c-0c54ef3bfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fff189b-040d-488c-ad1c-81b1e56f7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>대화의 빈 칸에 알맞은 것을 고르세요.\n",
      "A : Happy birthday!\n",
      "B : Oh, ____________\n",
      "\n",
      "1번 \"sorry\" 2번 \"thank you\" 3번 \"June first\"\n",
      " Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, I need to figure out what goes in the blank for each of these sentences. The sentences are:\n",
      "\n",
      "1. \"Sorry\"\n",
      "2. \"Thank you\"\n",
      "3. \"June first\"\n",
      "\n",
      "The context is that these are responses from a teacher to a student. So I should think about what a teacher might say in each situation.\n",
      "\n",
      "Starting with the first one: \"Sorry.\" A teacher might say \"Sorry\" in a few different contexts. Maybe if the student did something wrong, the teacher might say \"Sorry, I can't help you with that.\" Or if the teacher made a mistake, they might say \"Sorry for the confusion.\" But considering the context of a birthday message, maybe the teacher is responding to a student's birthday message. So perhaps it's \"Sorry\" as in \"Sorry, I can't come to your birthday party because...\" But I'm not sure. Alternatively, in a more casual context, a teacher might say \"Sorry for being late.\" Hmm, but in this case, the second part is \"June first,\" which might mean June is the first month or something. Wait, no, maybe \"June first\" is part of the message, so the teacher is replying to a student who said something like, \"My birthday is on June first.\" So the teacher might say, \"Sorry, I can't attend your birthday on June first.\" But I'm not entirely sure. Maybe it's just a fill-in-the-blank, so the first blank is \"Sorry,\" the second is \"Thank you,\" and the third is \"June first.\"\n",
      "\n",
      "Wait, but the problem is to fill in the blanks, so the sentences are:\n",
      "\n",
      "A: Happy birthday!\n",
      "B: Oh, ____________.\n",
      "\n",
      "So it's a two-part sentence where B is responding to A. So maybe the teacher is responding to the student's birthday message. So if the student says \"Happy birthday!\", the teacher might say \"Oh, I'm sorry I can't make it\" or something like that. But the structure here is \"Oh, ____________.\" So it's expecting a phrase that goes after \"Oh,\" like \"Oh, I'm sorry I can't come.\" But in the given options, the possible answers are \"sorry,\" \"thank you,\" and \"June first.\" So maybe it's more about the teacher responding in a way that's a bit more general. Alternatively, perhaps it's about the student's name. Wait, but the third option is \"June first,\" which might be the date.\n",
      "\n",
      "Wait, perhaps the problem is that the student wrote \"Happy birthday!\" and the teacher is replying. So the teacher might say, \"Oh, I'm sorry I can't attend your birthday on June first.\" So the first blank would be \"I'm sorry I can't attend your birthday on June first.\" But the options given are single words: sorry, thank you, June first. So maybe it's broken down into parts. Let me think again.\n",
      "\n",
      "Alternatively, maybe the teacher is saying, \"Oh, sorry, I can't come,\" but the options are individual words. So perhaps the first blank is \"I'm sorry,\" but that's not an option. The options are \"sorry,\" \"thank you,\" and \"June first.\" So maybe the teacher is saying \"Oh, I'm sorry, but I can't come on June first.\" So the blanks would be filled as \"I'm sorry, but I can't come on June first.\" But again, the options are single words.\n",
      "\n",
      "Alternatively, perhaps the teacher is responding to the student's birthday message, so the teacher is saying \"Oh, I'm sorry, I can't make it to your birthday.\" So the blank is \"I'm sorry, I can't make it.\" But the options are single words. So maybe the first blank is \"I'm sorry,\" but that's not an option. Alternatively, the teacher is just saying \"Sorry\" in a more general sense.\n",
      "\n",
      "Wait, maybe the problem is that the teacher is replying to a student who is asking for something, and the teacher is saying they can't do it. So the teacher says \"Oh, I'm sorry, I can't help you with that.\" So the blank would be \"I'm sorry, I can't help you with that.\" But again, the options are single words.\n",
      "\n",
      "Wait, perhaps the problem is that the teacher is responding to the student's birthday message, and the teacher is saying they can't attend. So the teacher says \"Oh, I'm sorry, I can't come to your birthday.\" So the blank is \"I'm sorry, I can't come.\" But the options are single words. So maybe the first blank is \"I'm sorry,\" but it's not an option. Alternatively, the teacher is just saying \"Sorry\" as a standalone.\n",
      "\n",
      "Alternatively, perhaps the teacher is saying \"Sorry, I can't do that.\" But again, the options are single words. Hmm, I'm getting confused. Maybe I should look at the options again. The options are:\n",
      "\n",
      "1. \"sorry\"\n",
      "\n",
      "2. \"thank you\"\n",
      "\n",
      "3. \"June first\"\n",
      "\n",
      "So the teacher is responding to the student's \"Happy birthday!\" So maybe the teacher is saying, \"Oh, I'm sorry I can't come to your birthday on June first.\" So the blanks would be \"I'm sorry I can't come on June first.\" But the options are individual words, so perhaps the first blank is \"I'm sorry,\" but that's not an option. Alternatively, the teacher is just saying \"Sorry\" as in \"Oh, sorry,\" but that might not make complete sense.\n",
      "\n",
      "Wait, maybe the teacher is responding to the student's birthday message by saying they can't attend, and the student's birthday is on June first. So the teacher says, \"Oh, I'm sorry, I can't come on June first.\" So the blanks would be filled with \"I'm sorry, I can't come on June first.\" But again, the options are single words.\n",
      "\n",
      "Alternatively, perhaps the teacher is just saying \"Sorry\" as a general response, but that seems less likely.\n",
      "\n",
      "Wait, maybe the problem is that the student is asking the teacher something, and the teacher is responding. For example, the student might have asked the teacher to help with something, and the teacher is saying they can't. So the teacher says, \"Oh, I'm sorry, I can't help you with that.\" So the blanks would be \"I'm sorry, I can't help you with that.\" But again, the options are single words.\n",
      "\n",
      "Alternatively, maybe the teacher is responding to a request and saying \"Thank you\" for something. So \"Oh, thank you for asking me.\" But the options are \"sorry,\" \"thank you,\" and \"June first.\"\n",
      "\n",
      "Wait, perhaps the teacher is responding to the student's birthday message by saying \"Thank you\" for the birthday wishes. So the teacher says, \"Oh, thank you for the birthday wishes.\" But the options are single words.\n",
      "\n",
      "I'm getting stuck here. Maybe the problem is that the teacher is replying to the student's message, and the blanks are to be filled with appropriate responses. So the teacher says:\n",
      "\n",
      "A: Happy birthday!\n",
      "B: Oh, [blank1] [blank2] [blank3]\n",
      "\n",
      "So the blanks are three separate words or phrases. So the first blank is \"I'm sorry,\" the second is \"I can't come,\" and the third is \"on June first.\" But the options given are \"sorry,\" \"thank you,\" and \"June first.\" So perhaps the blanks are filled as:\n",
      "\n",
      "B: Oh, [I'm sorry], [I can't come], [on June first].\n",
      "\n",
      "But the options are single words, so maybe the first blank is \"I'm sorry,\" which isn't an option. Alternatively, the teacher is saying \"Sorry, I can't come on June first.\" So the blanks would be \"Sorry, I can't come on June first.\" But again, the options are single words.\n",
      "\n",
      "Wait, maybe the teacher is just saying \"Sorry\" as a standalone, so the first blank is \"Sorry,\" the second is \"I can't come,\" and the third is \"on June first.\" But the options are three separate words: \"sorry,\" \"thank you,\" \"June first.\"\n",
      "\n",
      "Alternatively, perhaps the teacher is saying \"Sorry, I can't come on June first,\" so the blanks are \"Sorry, I can't come on June first.\" But the options are individual words.\n",
      "\n",
      "I think I'm overcomplicating this. Maybe the problem is that the teacher is replying to the student's birthday message, and the teacher is saying they can't attend. So the teacher says, \"Oh, I'm sorry, I can't come on June first.\" So the blanks are \"I'm sorry, I can't come on June first.\" But since the options are single words, perhaps the first blank is \"I'm sorry,\" but it's not an option.\n",
      "\n",
      "Alternatively, perhaps the teacher is just saying \"Sorry\" as a general response, so the first blank is \"Sorry,\" the second is \"I can't come,\" and the third is \"on June first.\" But the options are single words.\n",
      "\n",
      "Wait, maybe the problem is that the teacher is responding to the student's message, and the blanks are to be\n",
      "76.15404 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd0beac-e263-4201-a301-5e1efe7c9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": english_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1c248b-5541-4903-9571-61fa42a3cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>주어진 단어에 포함되는 단어를 모두 고르시오.\n",
      "주어진 단어 : \"Pet\"\n",
      "1번 \"cat\" 2번 \"picture\" 3번 \"rabbit\" 4번 \"toothpaste\"\n",
      "Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, so I have this question where I need to pick the word that includes \"pet\" among the given options: \"cat,\" \"picture,\" \"rabbit,\" and \"toothpaste.\" Hmm, let me think about this step by step.\n",
      "\n",
      "First, I need to understand what the question is asking. It says \"주어진 단어에 포함되는 단어를 모두 고르시오,\" which I think means \"pick the word that contains the given word.\" So, I have to find which of the four words contains \"pet\" as a part of it.\n",
      "\n",
      "Let me go through each option one by one.\n",
      "\n",
      "1. The first option is \"cat.\" Does \"cat\" contain \"pet\"? Well, \"cat\" is a standalone word, and \"pet\" is another word. \"Cat\" is an animal, while \"pet\" is something you have, like a dog or a fish. So, \"cat\" doesn't include \"pet\" within it. It's a different word.\n",
      "\n",
      "2. The second option is \"picture.\" Does \"picture\" have \"pet\" in it? Let me think. A picture can be of different things—a landscape, a person, an animal, etc. I don't think \"picture\" specifically includes \"pet\" as part of the word itself. It's more about the representation of something rather than containing another word.\n",
      "\n",
      "3. The third option is \"rabbit.\" Does \"rabbit\" contain \"pet\"? \"Rabbit\" is another animal, but again, it's a different word from \"pet.\" \"Pet\" refers to a type of animal you keep, while \"rabbit\" is another creature entirely. So, \"rabbit\" doesn't include \"pet.\"\n",
      "\n",
      "4. The fourth option is \"toothpaste.\" This seems a bit tricky. Toothpaste is a product used for cleaning teeth. Does \"toothpaste\" contain \"pet\"? I don't think so. \"Pet\" is an animal, and \"toothpaste\" is related to oral hygiene, not animals.\n",
      "\n",
      "Wait, maybe I'm missing something. Let me double-check. Could \"toothpaste\" somehow relate to a pet? Maybe if you have a pet, you might brush its teeth with toothpaste, but the word itself doesn't include \"pet.\" It's more about the product than the animal.\n",
      "\n",
      "So, after considering each option, none of the words contain \"pet\" within them. But the question says \"주어진 단어에 포함되는 단어를 모두 고르시오,\" which might mean to select all words that include \"pet.\" Since none of the options include \"pet,\" maybe the correct answer is none of them. However, the user might be expecting one of the options, so perhaps I made a mistake.\n",
      "\n",
      "Wait, maybe \"pet\" is part of \"toothpaste.\" Let me think about the letters. \"Toothpaste\" has 'p' and 'e,' but does it have \"pet\"? No, it's 't,' 'o,' 'o,' 't,' 'h,' 'p,' 'a,''s,' 't,' 'e.' So, \"toothpaste\" has 'p' and 't's but not the combination \"pet.\"\n",
      "\n",
      "I'm confused. Maybe the user made a mistake, or I'm overcomplicating it. Alternatively, perhaps \"toothpaste\" is the answer because \"tooth\" and \"paste\" are parts, but \"pet\" isn't in there. I think I should stick with the initial thought that none of the options include \"pet.\"\n",
      "\n",
      "Wait, maybe the user wants the word that is similar to \"pet.\" But no, the question is about inclusion of the word \"pet.\" So, I think the correct answer is that none of the options include \"pet.\" But the user might be expecting me to choose \"toothpaste\" because it's a product related to pets. However, the word itself doesn't include \"pet.\"\n",
      "\n",
      "Alternatively, perhaps the user meant to ask which word is related to pets, not which contains \"pet.\" In that case, \"toothpaste\" would make sense because people use toothpaste for their pets' dental care. But the wording says \"주어진 단어에 포함되는 단어를 모두 고르시오,\" which is more about inclusion of the word \"pet.\"\n",
      "\n",
      "I'm a bit stuck, but I think the safest answer is that none of the options include \"pet.\" But maybe the user expects me to say \"toothpaste\" because it's related. I'm not entirely sure, but I'll go with none of them.\n",
      "</think>\n",
      "\n",
      "The correct answer is that none of the given options include the word \"pet.\" However, if the task is to select a word related to pets, \"toothpaste\" might be the intended choice, as it is associated with pet care, though it doesn't contain the word \"pet.\"\n",
      "\n",
      "**Answer:** None of the given options include the word \"pet.\" If considering relatedness to pets, \"toothpaste\" might be the intended choice.\n",
      "41.28990 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e39b2-c305-47c2-8109-6e3fcefcc6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3931b3-40e6-4a85-aaab-57ff23e0a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_1 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f2af30-a16c-4f91-a7b7-09117666f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>동물 친구들이 곰에게 글을 읽을 때의 바른 자세에 대하여 말하고 있습니다. 토끼는 의자를 당겨서 앉아야 한다고 하였습니다. 기린은 허리를 곧게 펴야 한다고 하였습니다. 생쥐는 책과 눈의 거리를 알맞게 해야 한다고 하였습니다.\n",
      "\n",
      "1. 토끼가 곰에게 해 준 말은 무엇입니까?\n",
      "1번 \"허리를 곧게 펴야 한다.\" 2번 \"의자를 당겨서 앉아야 한다.\" 3번 \"다리를 쭉 펴고 읽어야 한다.\" 4번 \"책상 위에 엎드려 읽어야 한다.\" 5번 \"책과 팔의 거리를 알맞게 해야 한다.\"\n",
      "Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, I need to figure out what the correct answers are for the questions about how the animals sit and read properly. Let me go through each point one by one.\n",
      "\n",
      "First, the question is about what the hedgehog said to the raccoon. The options are:\n",
      "\n",
      "1. \"허리를 곧게 펴야 한다.\"\n",
      "2. \"의자를 당겨서 앉아야 한다.\"\n",
      "3. \"다리를 쭉 펴고 읽어야 한다.\"\n",
      "4. \"책상 위에 엎드려 읽어야 한다.\"\n",
      "5. \"책과 팔의 거리를 알맞게 해야 한다.\"\n",
      "\n",
      "So, the first part is about the hedgehog's advice to the raccoon. I remember that each animal has its own correct advice. The raccoon is supposed to sit properly on the chair. So, looking at the options, option 2 is \"의자를 당겨서 앉아야 한다.\" That sounds right because it's about sitting on a chair correctly.\n",
      "\n",
      "Next, the second part is about the reindeer's advice to the hedgehog. The reindeer said something about spreading its legs. The options again are the same. So, looking at the answers, I think the reindeer's advice is \"다리를 쭉 펴고 읽어야 한다.\" That makes sense because it's about spreading the legs when sitting.\n",
      "\n",
      "Then, the third part is about the mouse's advice to the squirrel. The mouse is supposed to adjust the distance from the book. So, looking at the options, the mouse's advice would be \"책과 팔의 거리를 알맞게 해야 한다.\" That fits because it's about the distance from the book.\n",
      "\n",
      "Wait, but I need to make sure I'm assigning the correct advice to each animal. Let me double-check:\n",
      "\n",
      "- Raccoon: sitting on the chair (option 2)\n",
      "- Reindeer: spreading legs (option 3)\n",
      "- Mouse: distance from book (option 5)\n",
      "\n",
      "So, the answers should be 2, 3, and 5 respectively.\n",
      "</think>\n",
      "\n",
      "The correct answers are:\n",
      "\n",
      "1. **2. 의자를 당겨서 앉아야 한다.** (Raccoon's advice to sit properly on the chair)\n",
      "2. **3. 다리를 쭉 펴고 읽어야 한다.** (Reindeer's advice to spread its legs while reading)\n",
      "3. **5. 책과 팔의 거리를 알맞게 해야 한다.** (Mouse's advice to adjust the distance from the book)\n",
      "\n",
      "These responses appropriately reflect the correct posture and behavior for each animal when reading.\n",
      "21.95716 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20bf1723-c169-4ccf-b408-64e098b562f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_2 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9621ee36-ad6a-4461-b831-588f3872f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>다음 중 자음자를 고르세요.\n",
      "1번 \"ㅏ\" 2번 \"ㅣ\" 3번 \"ㅁ\" 4번 \"ㅜ\" 5번 \"ㅛ\"\n",
      "Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, so I have to choose the correct consonant from the given options: 1. \"ㅏ\", 2. \"ㅣ\", 3. \"ㅁ\", 4. \"ㅜ\", 5. \"ㅛ\". I'm a bit confused because I remember that in Korean, vowels and consonants can sometimes look similar, but I need to figure out which one is a consonant.\n",
      "\n",
      "First, I'll try to recall the basic vowels in Korean. I think the main vowels are \"ㅏ\", \"ㅚ\", \"ㅟ\", \"ㅛ\", \"ㅜ\", \"ㅞ\", \"ㅠ\", \"ㄴ\", \"ㅣ\", and \"ㅇ\". Wait, but I'm not sure if \"ㄴ\" is considered a vowel or consonant. I think it's a vowel because it's used in sounds like \"geun\" or something.\n",
      "\n",
      "Looking at the options, \"ㅏ\" is a vowel, right? It's like the 'a' sound. \"ㅣ\" is a consonant, I believe. It's like the 'l' sound. \"ㅁ\" is another consonant, similar to'm'. \"ㅜ\" is a vowel, it's like the 'u' sound. \"ㅛ\" is also a vowel, like the 'o' sound.\n",
      "\n",
      "So, among the options, the consonants are \"ㅣ\" and \"ㅁ\". Wait, but the question is asking to choose one consonant, right? Or is it to choose all consonants? The initial question says \"다음 중 자음자를 고르세요\", which I think means \"choose a consonant from the following\". So I need to pick one.\n",
      "\n",
      "Hmm, maybe I should double-check each one. \"ㅏ\" is a vowel, so it's out. \"ㅣ\" is consonant 'l' sound, yes. \"ㅁ\" is consonant'm' sound. \"ㅜ\" is vowel 'u', so no. \"ㅛ\" is vowel 'o', so no. So the consonants are 2 and 3. But the question says \"다음 중 자음자를 고르세요\", which might mean select all consonants or choose one. Maybe in the original context, but in the translation, it says \"select a consonant\", so perhaps only one.\n",
      "\n",
      "Wait, maybe the user is asking to select the consonant sounds. So the correct answer would be either \"ㅣ\" or \"ㅁ\". But in the initial problem, it's presented as a single selection. So maybe the correct consonant is either 2 or 3. I think both are consonants, but perhaps the question expects a specific one. Maybe the user is trying to test if I know the difference between vowels and consonants in Korean.\n",
      "\n",
      "Alternatively, perhaps the user made a typo and meant to provide a single choice, but the options are 2 and 3. So I should probably explain that both are consonants, but if I have to pick one, maybe either is correct, but I should clarify.\n",
      "\n",
      "Wait, no, the initial problem seems to be asking to choose one. So perhaps the correct answer is either 2 or 3. But I need to confirm. Let me think again. \"ㅣ\" is 'l' and \"ㅁ\" is'm', both consonants. So both are correct. Maybe the question expects multiple answers, but the user's translation says \"select one\". I'm a bit confused.\n",
      "\n",
      "Wait, perhaps the user wants to know which of the given are consonants, so I should list them both. So the correct answers are 2 and 3. But in the initial problem, it's presented as five options, so maybe it's a multiple-choice where I have to select all that apply. But the user's instruction says \"Please tell me the correct answer\", implying a single answer.\n",
      "\n",
      "Hmm, maybe I should explain that both 2 and 3 are consonants, but if I have to pick one, either is correct. Alternatively, perhaps the correct answer is both, but I should mention that.\n",
      "\n",
      "Wait, no, looking back, the user wrote: \"다음 중 자음자를 고르세요.\" which translates to \"choose a consonant from the following\". So it's singular, meaning one. But in the options, there are two consonants. Maybe the correct answer is both, but the user expects to know which ones are consonants. So perhaps the correct answer is 2 and 3, but the way the question is phrased, it's singular.\n",
      "\n",
      "I think I need to clarify, but since I can't ask, I'll proceed by explaining that both 2 and 3 are consonants, but if I have to choose one, both are correct. Alternatively, maybe the question is to select all consonants, but the user's translation is unclear.\n",
      "\n",
      "Wait, perhaps the user wants to know which of the five are consonants, and the correct ones are 2 and 3. So the answer is both. So I should say that 2 and 3 are consonants, and explain why the others are vowels.\n",
      "\n",
      "I think that's the way to go. So in the explanation, I'll list the consonants and explain each one.\n",
      "</think>\n",
      "\n",
      "The correct consonants in the given options are 2 (ㅣ) and 3 (ㅁ). \n",
      "\n",
      "- \"ㅣ\" represents the consonant 'l'.\n",
      "- \"ㅁ\" represents the consonant'm'.\n",
      "\n",
      "Both are consonants, and the other options are vowels.\n",
      "46.12899 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "112a7b52-4fd4-4152-a668-1f20602fc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": korean_3 + \"Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba999c90-3da8-41b1-bf59-d6c57c473d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant chatbot.<｜User｜>나무 나무 무슨 나무\n",
      "가자 가자 감나무\n",
      "배가 아파 배나무\n",
      "바람 솔솔 _______\n",
      "\n",
      "빈칸에 들어갈 말로 가장 알맞는 것은 무엇입니까?\n",
      "1번 \"소나무\" 2번 \"감나무\" 3번 \"배나무\" 4번 \"사과나무\" 5번 \"포도나무\"\n",
      "Speak as a teacher speaks to a student. Please tell me the correct answer to the problem and provide an appropriate explanation. Please speak Korean.<｜Assistant｜><think>\n",
      "Okay, so I'm trying to figure out what goes into the blank in the sentence: \"바람 솔솔 _______. \" The options are \"소나무,\" \"감나무,\" \"배나무,\" \"사과나무,\" and \"포도나무.\" Let me break this down step by step.\n",
      "\n",
      "First, I need to understand the structure of the sentence. The phrase is \"바람 솔솔 _______.\" In Korean, \"바람\" is the word for wind, and \"솔솔\" seems to mean something like \"blowing\" or \"blowing through.\" So the sentence is like \"The wind is blowing through something.\" So the blank is where the object that the wind is blowing through is supposed to go.\n",
      "\n",
      "Now, looking at the options: \"소나무,\" \"감나무,\" \"배나무,\" \"사과나무,\" and \"포도나무.\" All of these are types of trees or plants. Let me think about what each one means.\n",
      "\n",
      "\"소나무\" is pine tree. \"감나무\" is a type of tree that produces fruit, maybe something like persimmons? \"배나무\" is the mulberry tree. \"사과나무\" is the pear tree, and \"포도나무\" is the grapevine or the bamboo? Wait, no, \"포도나무\" is actually the bamboo, but in Korean, \"포도\" is grape, so \"포도나무\" might mean the grapevine, but I'm not entirely sure.\n",
      "\n",
      "Wait, no, \"포도나무\" is actually the bamboo, but in some contexts, it can also refer to the bamboo grove or bamboo in general. But the others are all types of trees that bear fruits. So \"감나무\" might be a tree that has fruits, maybe like figs? Or persimmons? I'm a bit confused.\n",
      "\n",
      "Wait, \"감나무\" is more specifically the persimmon tree. \"배나무\" is mulberry, \"사과나무\" is pear, and \"소나무\" is pine. So the sentence is saying the wind is blowing through a type of tree. So which of these makes sense in the sentence.\n",
      "\n",
      "In the first part, it's \"바람 솔솔,\" so the structure is \"wind blowing (through something).\" So we need a tree that's commonly associated with the wind. Maybe the sentence is part of a poem or a metaphor.\n",
      "\n",
      "Looking at the options, \"감나무\" is a tree that's known for its strong, dense wood, so the wind might be blowing through a \"감나무\" as in the wind moving through a dense forest of persimmon trees. Or maybe \"바람\" is the wind, and \"솔솔\" is like \"through.\" So the sentence would be \"The wind is blowing through the persimmon tree.\"\n",
      "\n",
      "Alternatively, \"배나무\" is mulberry, which also can be used in metaphorical contexts. Or \"포도나무\" as bamboo, but that might not fit as well in this structure.\n",
      "\n",
      "Wait, but \"감나무\" is a type of tree, so the sentence would make sense as \"바람이 감나무를 통해吹고 있다\" which would mean \"The wind is blowing through the persimmon tree.\"\n",
      "\n",
      "Alternatively, \"바람이 배나무를 통해吹고 있다\" would be \"The wind is blowing through the mulberry tree.\"\n",
      "\n",
      "But I think \"감나무\" is more likely because it's a more common expression. Or maybe \"솔솔\" is a verb meaning to blow, so the sentence is \"바람이 솔솔 _______,\" so the blank is filled with the object that the wind is blowing through.\n",
      "\n",
      "Wait, maybe the sentence is more like \"바람이 솔솔 [something]을 통해 지나가는 것.\" So the something is the object. So the options are trees, so which tree is more likely to be associated with the wind blowing through.\n",
      "\n",
      "I think \"감나무\" is a good fit because it's a type of tree, and the sentence would make sense as \"바람이 감나무를 통해吹고 있다.\" So the answer is \"감나무.\"\n",
      "\n",
      "Wait, but the options given are 1. 소나무, 2. 감나무, 3. 배나무, 4. 사과나무, 5. 포도나무.\n",
      "\n",
      "So the correct one is 2. 감나무.\n",
      "\n",
      "I think that's the right answer because \"감나무\" is a tree, and the sentence structure is \"wind blowing through the tree.\"\n",
      "</think>\n",
      "\n",
      "The correct answer is \"감나무.\"\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Understanding the Sentence Structure:** The sentence is \"바람 솔솔 _______.\" In Korean, \"바람\" means wind, and \"솔솔\" translates to \"blowing through.\" Thus, the sentence is asking for the object through which the wind is blowing.\n",
      "\n",
      "2. **Analyzing the Options:**\n",
      "   - \"소나무\" (pine tree)\n",
      "   - \"감나무\" (persimmon tree)\n",
      "   - \"배나무\" (mulberry tree)\n",
      "   - \"사과나무\" (pear tree)\n",
      "   - \"포도나무\" (bamboo)\n",
      "\n",
      "3. **Contextual Fit:** All options are trees, but \"감나무\" is a common choice in such contexts, often used in expressions about the wind blowing through trees.\n",
      "\n",
      "4. **Conclusion:** \"감나무\" fits best as it is a suitable object for the wind to blow through, making the sentence grammatically and contextually appropriate.\n",
      "\n",
      "**Answer:** 2. 감나무\n",
      "45.51405 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=2000,\n",
    ")\n",
    "print(sequences[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cacf25-2cf4-4f8d-890c-78ad87fe7fab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07f9042-e696-46c4-ac6b-ca76fc04e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674ba8b3-0228-4006-b6a5-bd845131bc09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 08:48:50 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit with actual GPU utilization = 69.06%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.69 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.74 GB. Also swap space = 6 GB.\n",
      "INFO 02-25 08:48:57 config.py:549] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-25 08:48:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 02-25 08:48:58 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-25 08:48:58 model_runner.py:1110] Starting to load model unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit...\n",
      "INFO 02-25 08:48:58 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W225 08:48:58.210021065 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 08:48:59 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9445f356d3f5424aa788c4ee62853055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f89146e80774093883318956a56f553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 08:49:01 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 02-25 08:49:01 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-25 08:49:02 worker.py:267] Memory profiling takes 0.84 seconds\n",
      "INFO 02-25 08:49:02 worker.py:267] the current vLLM instance can use total_gpu_memory (15.69GiB) x gpu_memory_utilization (0.69) = 10.83GiB\n",
      "INFO 02-25 08:49:02 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.89GiB; the rest of the memory reserved for KV Cache is 4.30GiB.\n",
      "INFO 02-25 08:49:02 executor_base.py:111] # cuda blocks: 2200, # CPU blocks: 3072\n",
      "INFO 02-25 08:49:02 executor_base.py:116] Maximum concurrency for 512 tokens per request: 68.75x\n",
      "INFO 02-25 08:49:04 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 27/27 [00:14<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 08:49:19 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.68 GiB\n",
      "INFO 02-25 08:49:19 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 512 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "model_id = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c86c781-fef7-47c9-861a-63c4b83972d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f1953f3c49438e94f6630827958716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b3500c-6cd5-42e4-a22e-624930bee622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f91e59ddedc4f82a37b6d579a3e363e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dd912ca15e43c183c3cbed289726ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f834f1f0a84c3da160723ade0c5ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356bbc40055e4b5ebf9b08f56be0350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6af5d8-5c0a-465e-979f-e54495639bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.040700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.10096 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.5f} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312]",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
